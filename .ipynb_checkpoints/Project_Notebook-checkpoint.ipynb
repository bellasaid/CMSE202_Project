{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Movie Likes and Dislikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All important imports go here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we are attempting to view different models' ability to predict likes and dislikes of movies. Below we have downloaded a dataframe from Kaggle that includes different information about movies from 1986-2016. The information includes things such as genre, company, the name of the movie, runtime, etc. What concerns us most is the score column that is included in this dataframe. The movies were each given a score that was ranked out of 10, 10 being the best, 1 being the worst. We want to utilize these scores to be able to build and train a model to predict movies scores accurately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('movies.csv', encoding = 'latin-1') #Read in this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "movies.head() #Display dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary columns \n",
    "movies = movies.drop(columns =['country', 'rating', 'released', 'votes', 'writer', 'director', 'star'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_score = movies['score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Binarizing the movie ratings into zeros and ones for easy classification later on\n",
    "movies['score'][movies['score'] < avg_score]= 0\n",
    "movies['score'][movies['score'] > avg_score]= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Doing a linear regression to find values to replace 0.0 in the budget column \n",
    "x = movies['gross']\n",
    "y = movies['budget']\n",
    "\n",
    "x_constant = sm.add_constant(x)\n",
    "gross_budget_model = sm.OLS(y, x_constant)\n",
    "results = gross_budget_model.fit()\n",
    "print(\"Intercept and slope are:\", results.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = results.params[0]\n",
    "b = results.params[1]\n",
    "#Replacing budgets of 0 with the budget values calculated in linear regression model\n",
    "for i in range(movies.shape[0]):\n",
    "    if movies['budget'][i] ==0.0:\n",
    "        gross_val = movies['gross'][i]\n",
    "        y = m*gross_val + b\n",
    "        movies['budget'].iloc[i] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing OneHotEncoder for genre labels, this allows genres to be in zeros and ones so we can use them as features\n",
    "#since they are no longer strings\n",
    "encoder = OneHotEncoder()\n",
    "genre = movies['genre']\n",
    "genre_np = genre.to_numpy()\n",
    "genre_ary = encoder.fit_transform(genre_np.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "genre_df = pd.DataFrame(genre_ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df = genre_df.rename({0:'Action', 1:'Adventure', 2:'Animation', 3:'Biography', 4:'Comedy', 5:'Crime', \n",
    "                            6:'Drama', 7:'Family', 8:'Fantasy', 9:'Horror', 10:'Musical', 11:'Mystery', 12:'Romance', \n",
    "                            13:'Sci-Fi', 14:'Thriller', 15:'War', 16:'Western'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "movies_encoded = pd.concat([movies, genre_df], axis =1)\n",
    "movies_encoded.head() #OneHotEncoder worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_df = movies_encoded.drop(columns=['genre', 'company', 'name'], axis=1)\n",
    "cleaned_df.head() #Cleaned dataframe ready for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing feature spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Visualizing features to see if there are any patterns present before we start modeling\n",
    "features = cleaned_df.drop(columns=['score'], axis =1)\n",
    "labels = cleaned_df['score']\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.subplot(321)\n",
    "plt.scatter(features['budget'], features['gross'], c=labels)\n",
    "plt.xlabel('Movie Budget')\n",
    "plt.ylabel('Movie Gross')\n",
    "plt.title('Budget vs. Gross')\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.scatter(features['runtime'], features['gross'], c=labels)\n",
    "plt.xlabel('Movie Runtime')\n",
    "plt.ylabel('Movie Gross')\n",
    "plt.title('Runtime vs. Gross')\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.scatter(features['year'], features['gross'], c=labels)\n",
    "plt.xlabel('Release Year')\n",
    "plt.ylabel('Movie Gross')\n",
    "plt.title('Release Year vs. Gross')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty hard to interpret. We can try scaling the features next to see if the visualization with be a little better than what is shown above. In previous In-class assignments, we have scaled features to be able to interpret and work with their visualizations in a much easier way. We will test this with our data next and see if it changes anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_scaled = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Attempting the same visualization with scaled features\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.subplot(321)\n",
    "plt.scatter(features_scaled.iloc[:, 0], features_scaled.iloc[:, 1], c=labels)\n",
    "plt.xlabel('Movie Budget')\n",
    "plt.ylabel('Movie Gross')\n",
    "plt.title('Budget vs. Gross')\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.scatter(features_scaled.iloc[:, 2], features_scaled.iloc[:, 1], c=labels)\n",
    "plt.xlabel('Movie Runtime')\n",
    "plt.ylabel('Movie Gross')\n",
    "plt.title('Runtime vs. Gross')\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.scatter(features_scaled.iloc[:, 4], features_scaled.iloc[:, 1], c=labels)\n",
    "plt.xlabel('Release Year')\n",
    "plt.ylabel('Movie Gross')\n",
    "plt.title('Release Year vs. Gross')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, scaling the features did little to help us visualize the relationship between the features. It still is just as hard to interpret the visualization with scaled features as it was before the features were scaled. Now below just to see more visualizations, we can test the plots of the OneHotEncoder columns that we created for the different movie genres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Attempting to visualize OneHotEncoder columns \n",
    "#For this visualization we chose features that might seem to having overlapping genres, perhaps this might influence\n",
    "# their ratings/score\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.subplot(321)\n",
    "plt.scatter(features['Action'], features['Adventure'], c=labels)\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Adventure')\n",
    "plt.title('Action vs. Adventure')\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.scatter(features['Comedy'], features['Romance'], c=labels)\n",
    "plt.xlabel('Comedy')\n",
    "plt.ylabel('Romance')\n",
    "plt.title('Comedy vs. Romance')\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.scatter(features['Horror'], features['Mystery'], c=labels)\n",
    "plt.xlabel('Horror')\n",
    "plt.ylabel('Mystery')\n",
    "plt.title('Horror vs. Mystery')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the fact that there is only one genre associated with each movie, it makes it a lot harder to visualize the data in this manner. Perhaps if we did a heatmap of the correlations it might be easier to identify if there are any columns in the dataset that might have correlations with one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(features.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the color hue indicator on the right hand side of the figure we can see at a glance that none of the features have higher than 0.2 of a positive correlation with another given column. Some higher correlations present in the heatmap might be between runtime and gross as well as gross and year. But, as we saw in the feature space visualizations above, it's very difficult to be able to see any correlations or relationships between the features in the plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cleaned_df.drop(columns=['score'], axis =1)\n",
    "labels = cleaned_df['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(features, labels, train_size =.75,\n",
    "                                                                         test_size = .25, random_state =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create a PCA model using kernel rbf along with default C and gamma values. We will also includes 10 components for this PCA which is fewer than the number of features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating and fiting PCA model, first using 10 components\n",
    "pca = PCA(n_components=10, whiten=True)\n",
    "pca = pca.fit(train_vectors)\n",
    "\n",
    "##Now transforming train and test vectors into PCA train and test vectors\n",
    "pca_train_vectors = pca.transform(train_vectors)\n",
    "pca_test_vectors = pca.transform(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now fitting model using SVC with kernel rbf with default C and gamma values\n",
    "pca_svm = SVC(kernel ='rbf', C=10, gamma = 0.1)\n",
    "pca_model = pca_svm.fit(pca_train_vectors, train_labels)\n",
    "pca_ypred = pca_model.predict(pca_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now printing metrics to look at accuracy of model\n",
    "print('The confusion matrix is \\n', confusion_matrix(test_labels, pca_ypred))\n",
    "print('The classification report is \\n', classification_report(test_labels, pca_ypred))\n",
    "print('The accuracy score is \\n', accuracy_score(test_labels, pca_ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will attempt to make a PCA that has the same number of components as features in our dataset. We will then see if the accuracy scores change at all with this change in number of components. We will keep the kernel as rbf and default C and gamma values, same as the first PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating and fiting PCA model, now using 21 components, which is equal to the number of features in our dataset\n",
    "pca = PCA(n_components=21, whiten=True)\n",
    "pca = pca.fit(train_vectors)\n",
    "\n",
    "##Now transforming train and test vectors into PCA train and test vectors\n",
    "pca_train_vectors = pca.transform(train_vectors)\n",
    "pca_test_vectors = pca.transform(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now fitting model using SVC with kernel rbf with default C and gamma values\n",
    "pca_svm = SVC(kernel ='rbf', C=10, gamma = 0.1)\n",
    "pca_model = pca_svm.fit(pca_train_vectors, train_labels)\n",
    "pca_ypred21 = pca_model.predict(pca_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now printing metrics to look at accuracy of model\n",
    "print('The confusion matrix is \\n', confusion_matrix(test_labels, pca_ypred21))\n",
    "print('The classification report is \\n', classification_report(test_labels, pca_ypred21))\n",
    "print('The accuracy score is \\n', accuracy_score(test_labels, pca_ypred21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The accuracy score of the PCA with 10 components is \\n', accuracy_score(test_labels, pca_ypred))\n",
    "print('The accuracy score of the PCA with 21 components is \\n', accuracy_score(test_labels, pca_ypred21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the cell above, the accuracy score for both PCAs, one with 10 components and one with 21 components, are pretty much equal. If we look more closely we can see that the accuracy for 21 components is slightly lower than the accuracy for 10 components. This might indicate that we might need to use fewer components and to get a better accuracy since after 10 components the accuracies are the same. Perhaps once we pass 10 components we might be overfitting the data, so it might be a good idea to try a PCA with fewer components, for example 4. The number of features that do not include the OneHotEncoder we did of the genres is four features, so maybe that would be a good number less than 10 that we could try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying a PCA with 4 components to see if we were overfitting with 10 components\n",
    "pca = PCA(n_components=4, whiten=True)\n",
    "pca = pca.fit(train_vectors)\n",
    "\n",
    "##Now transforming train and test vectors into PCA train and test vectors\n",
    "pca_train_vectors = pca.transform(train_vectors)\n",
    "pca_test_vectors = pca.transform(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now fitting model using SVC with kernel rbf with default C and gamma values\n",
    "pca_svm = SVC(kernel ='rbf', C=10, gamma = 0.1)\n",
    "pca_model = pca_svm.fit(pca_train_vectors, train_labels)\n",
    "pca_ypred4 = pca_model.predict(pca_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now printing metrics to look at accuracy of model\n",
    "print('The confusion matrix is \\n', confusion_matrix(test_labels, pca_ypred4))\n",
    "print('The classification report is \\n', classification_report(test_labels, pca_ypred4))\n",
    "print('The accuracy score is \\n', accuracy_score(test_labels, pca_ypred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The accuracy score for a PCA with 4 components is \\n', accuracy_score(test_labels, pca_ypred4))\n",
    "print('The accuracy score of the PCA with 10 components is \\n', accuracy_score(test_labels, pca_ypred))\n",
    "print('The accuracy score of the PCA with 21 components is \\n', accuracy_score(test_labels, pca_ypred21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This didn't work either. The accuracy with 4 components seems to be the same as the accuracy with 21 components. However, one interesting thing to look at is the confusion matrix for all three models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('The confusion matrix for a PCA with 4 components is \\n', confusion_matrix(test_labels, pca_ypred4))\n",
    "print('The confusion matrix for a PCA with 21 components is \\n', confusion_matrix(test_labels, pca_ypred21))\n",
    "print('The confusion matrix for a PCA with 10 components is \\n', confusion_matrix(test_labels, pca_ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above the confusion matrix has the higher accuracy of true positives for the PCA with 4 components and the lowest number of true positive for the PCA with 10 components. Overall, there are still a lot of false positives and false negatives present in all of the confusion matrices regardless of component number so we still have a really low accuracy for all versions of the PCA. One reason that we might have low accuracy in our models is because we have feature spaces that do not have positive correlations with one another, and therefore they do not have a strong relationship with one another. Since our dataset does not have strong relationships between different columns it would make it a lot harder to build an accurate model using the features that we currently have.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
